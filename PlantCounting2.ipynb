{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcff413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b79baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_PATH = \"Plant Counting/train/train.tfrecord\"\n",
    "VALID_PATH = \"Plant Counting/valid/valid.tfrecord\"\n",
    "TEST_PATH = \"Plant Counting/test/test.tfrecord\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "IMG_SIZE = (224, 224)\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6cc280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tfrecord(example):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'count': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
    "    image = tf.image.resize(image, IMG_SIZE) / 255.0\n",
    "    label = tf.cast(example['count'], tf.float32)\n",
    "    return image, label\n",
    "\n",
    "def load_dataset(path):\n",
    "    dataset = tf.data.TFRecordDataset(path)\n",
    "    dataset = dataset.map(decode_tfrecord, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa47419",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(TRAIN_PATH)\n",
    "valid_ds = load_dataset(VALID_PATH)\n",
    "test_ds = load_dataset(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b9070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = applications.MobileNetV2(input_shape=IMG_SIZE + (3,), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e68082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node ParseSingleExample/ParseExample/ParseExampleV2 defined at (most recent call last):\n<stack traces unavailable>\nFeature: count (data type: int64) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_8586]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m callbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m callbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amany\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\amany\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node ParseSingleExample/ParseExample/ParseExampleV2 defined at (most recent call last):\n<stack traces unavailable>\nFeature: count (data type: int64) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_8586]"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit(train_ds, validation_data=valid_ds, epochs=EPOCHS, callbacks=[early_stop, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b637b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_ds)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d474886",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in test_ds:\n",
    "    preds = model.predict(images)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(preds.flatten())\n",
    "\n",
    "plt.scatter(y_true, y_pred)\n",
    "plt.xlabel('Actual Count')\n",
    "plt.ylabel('Predicted Count')\n",
    "plt.title('Actual vs Predicted Plant Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((np.array(y_true) - np.array(y_pred)) ** 2))\n",
    "r2 = 1 - (np.sum((np.array(y_true) - np.array(y_pred)) ** 2) / np.sum((np.array(y_true) - np.mean(y_true)) ** 2))\n",
    "\n",
    "print(\"Test RMSE:\", rmse)\n",
    "print(\"Test RÂ²:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('/kaggle/working/Plant_Counting_Upgraded_Model.keras')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": [
    {
     "file_id": "1pJeb8C_xShILgzL97aYuq2cF-AVTz98I",
     "timestamp": 1745340290242
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
